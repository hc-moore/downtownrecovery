{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 1 random point for each 100 jobs in every block group, and apply HDBSCAN to identify clusters in each city. Then select largest/most central cluster in each city to be downtown. Do this for US and Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install/load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install folium matplotlib mapclassify \n",
    "# import sys\n",
    "# !{sys.executable} -m pip install contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import matplotlib\n",
    "import mapclassify\n",
    "import numpy as np\n",
    "import shapely\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import hdbscan\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine US and Canada data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/jpg23/data/downtownrecovery/lehd_new_downtowns/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada = gpd.read_file(data_path + \"canada_DA_jobs.geojson\") # Canada\n",
    "# us_bg = gpd.read_file(data_path + \"cities_lehd_jobs_blockgroup.geojson\") # US - block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# canada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_sf = gpd.read_file(data_path + \"reprojected_canada_DA.geojson\")[['DAUID', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_sf.DAUID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_newgeom = canada_sf.merge(canada.drop('geometry', axis=1), left_on='DAUID', right_on='id', how='inner').drop(columns = ['DAUID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_newgeom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_newgeom.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# us_bg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_newgeom.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# us_bg.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reproject so they're in the same CRS\n",
    "# us_bg_reproj = us_bg.to_crs('EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_bg_reproj.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_bg_reproj.crs == canada_newgeom.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gut check - are jobs_per_sq_meter relatievly similar for US & Canada?\n",
    "# print(canada_newgeom.jobs_per_sq_meter.mean())\n",
    "# us_bg_reproj.jobs_per_sq_meter.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(canada_newgeom.jobs_per_sq_meter.median())\n",
    "# us_bg_reproj.jobs_per_sq_meter.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stack the datasets\n",
    "# canada_final = canada_newgeom.rename(columns={\"CMANAME\": \"place\"})[['id', 'total_jobs', 'jobs_per_sq_meter', 'place', 'geometry']]\n",
    "\n",
    "# us_bg_final = us_bg_reproj.rename(columns={\"block_group\":\"id\", \"city\":\"place\"})[['id', 'total_jobs', 'jobs_per_sq_meter', 'place', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([us_bg_final, canada_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.place.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the Quebec part of Ottawa\n",
    "# df = df[df['place']!='Ottawa - Gatineau (partie du QuÃ©bec / Quebec part)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.place.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at Tampa as an example\n",
    "# tampa = df[df['place']=='Tampa FL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tampa.explore(\n",
    "#     tooltip=\"id\",\n",
    "#     tiles=\"CartoDB positron\",\n",
    "#     style_kwds=dict(color=\"black\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In each block group/dissemination area, randomly scatter 1 point for every 100 jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scatter random points in polygon\n",
    "\n",
    "# def Random_Points_in_Bounds(polygon, number):   \n",
    "#     minx, miny, maxx, maxy = polygon.bounds\n",
    "#     x = np.random.uniform(minx, maxx, number*4)\n",
    "#     y = np.random.uniform(miny, maxy, number*4)\n",
    "#     gdf_poly = gpd.GeoDataFrame(index=[\"myPoly\"], geometry=[polygon])\n",
    "#     df = pd.DataFrame()\n",
    "#     df['points'] = list(zip(x,y))\n",
    "#     df['points'] = df['points'].apply(shapely.geometry.Point)\n",
    "#     gdf_points = gpd.GeoDataFrame(df, geometry='points')\n",
    "#     Sjoin = gpd.sjoin(gdf_points, gdf_poly, op=\"within\", how='left')\n",
    "#     pnts_in_poly = gdf_points[Sjoin.index_right=='myPoly']\n",
    "#     return pnts_in_poly['points'].tolist()[0:number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # divide # of total jobs by 100\n",
    "# df['jobs_hundreds'] = round((df['total_jobs']/100), 0)\n",
    "\n",
    "# # filter to only block groups / dissemination areas where there are at least 100 jobs\n",
    "# df_100 = df[df['jobs_hundreds']>0].copy()\n",
    "\n",
    "# # create randomly scattered points in each block group / dissemination area\n",
    "# df_100['points'] = df_100[['geometry', 'jobs_hundreds']].apply(lambda x: Random_Points_in_Bounds(x[0], int(x[1])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a row for each set of points\n",
    "# df_100_points = df_100.explode('points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_100_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df_100_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize format for dbscan\n",
    "# df_dbscan = gpd.GeoDataFrame(df_100_points[['id', 'points', 'jobs_per_sq_meter', 'place']].copy(), geometry='points')\n",
    "# df_dbscan['point_lon'] = df_dbscan['points'].x\n",
    "# df_dbscan['point_lat'] = df_dbscan['points'].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_dbscan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dbscan.to_file(data_path + 'blockgroup_random_pts.geojson', driver='GeoJSON')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbscan = gpd.read_file(data_path + \"blockgroup_random_pts.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply HDBSCAN to determine clusters in each city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [hdbscan documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html) and [documentation on parameter selection](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbscan_notnull = df_dbscan[df_dbscan['point_lon'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what % of rows were not NA?\n",
    "df_dbscan_notnull.shape[0]/df_dbscan.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize lat/long values\n",
    "X = StandardScaler().fit_transform(df_dbscan_notnull[['point_lon', 'point_lat']].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=25,\n",
    "                           core_dist_n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of labels\n",
    "\n",
    "np.max(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# append results to original database\n",
    "\n",
    "df_dbscan_notnull['cluster'] = clusterer.labels_\n",
    "df_dbscan_notnull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbscan_notnull.cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dbscan_notnull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore points in specific cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of all points in city\n",
    "def explore_pts(city_name):\n",
    "    pts_only_city = df_dbscan_notnull[df_dbscan_notnull['place']==city_name].set_crs(4326)\n",
    "    \n",
    "#     ax = pts_only_city.plot(figsize=(9, 9), alpha=0.5)\n",
    "#     cx.add_basemap(ax, source=cx.providers.CartoDB.Positron, crs=4326)\n",
    "\n",
    "    return(pts_only_city.explore(\n",
    "        tiles=\"CartoDB positron\",\n",
    "        style_kwds=dict(opacity=.5, fillOpacity=.5)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_pts('San Francisco CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_pts('Nashville-Davidson metropolitan government (balance) TN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_pts('Portland OR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore clusters in specific cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-clustered points and set CRS\n",
    "clusters_only = df_dbscan_notnull[df_dbscan_notnull[\"cluster\"]!=-1].set_crs(4326)\n",
    "clusters_only['cluster'] = clusters_only['cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactive map of clusters\n",
    "# see https://matplotlib.org/stable/users/explain/colors/colormaps.html for cmap options\n",
    "\n",
    "def explore_clusters(city_name):\n",
    "    \n",
    "    clusters_only_city = clusters_only[clusters_only['place']==city_name]\n",
    "    \n",
    "    return(clusters_only_city.explore(\n",
    "        column=\"cluster\",\n",
    "        tiles=\"CartoDB positron\",\n",
    "        style_kwds=dict(opacity=.5, fillOpacity=.5),\n",
    "        cmap='Spectral'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters('San Francisco CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters('Toronto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters('Calgary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters('Albuquerque NM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters('New York NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters('Philadelphia PA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore_clusters('Nashville-Davidson metropolitan government (balance) TN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune HDBSCAN parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this link](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html) and [this Stack Overflow post](https://stackoverflow.com/questions/67898039/hdbscan-difference-between-parameters) for a helpful explanation of how the parameters work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_tuned(which_min_cluster_size, which_min_samples, which_cluster_selection_epsilon):\n",
    "    \n",
    "    clusterer_tuned = hdbscan.HDBSCAN(min_cluster_size = which_min_cluster_size,\n",
    "                                      min_samples = which_min_samples,\n",
    "                                      cluster_selection_epsilon = which_cluster_selection_epsilon,\n",
    "                                      core_dist_n_jobs=1)\n",
    "    \n",
    "    clusterer_tuned.fit(X)\n",
    "    \n",
    "    # append results to original database\n",
    "    df_dbscan_notnull_tuned = df_dbscan_notnull\n",
    "    df_dbscan_notnull_tuned['cluster'] = clusterer_tuned.labels_\n",
    "    df_dbscan_notnull_tuned.head()\n",
    "\n",
    "    # Filter out non-clustered points and set CRS\n",
    "    clusters_only_tuned = df_dbscan_notnull_tuned[df_dbscan_notnull_tuned[\"cluster\"]!=-1].set_crs(4326)\n",
    "    clusters_only_tuned['cluster'] = clusters_only_tuned['cluster'].astype(str)\n",
    "    \n",
    "    return(clusters_only_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactive map of clusters\n",
    "# see https://matplotlib.org/stable/users/explain/colors/colormaps.html for cmap options\n",
    "\n",
    "def explore_clusters_tuned(tuned_df, city_name):\n",
    "    \n",
    "    df = tuned_df[tuned_df['place']==city_name]\n",
    "    \n",
    "    return(df.explore(\n",
    "        column=\"cluster\",\n",
    "        tiles=\"CartoDB positron\",\n",
    "        style_kwds=dict(opacity=.5, fillOpacity=.5),\n",
    "        cmap='Spectral'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100_25_0 = create_clusters_tuned(which_min_cluster_size = 100, \n",
    "                                    which_min_samples = 25, \n",
    "                                    which_cluster_selection_epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters_tuned(df_100_25_0, 'San Francisco CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100_100_0 = create_clusters_tuned(which_min_cluster_size = 100, \n",
    "                                     which_min_samples = 100, \n",
    "                                     which_cluster_selection_epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore_clusters_tuned(df_100_100_0, 'San Francisco CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_100_100_5 = create_clusters_tuned(which_min_cluster_size = 100, \n",
    "                                     which_min_samples = 100, \n",
    "                                     which_cluster_selection_epsilon = .00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore_clusters_tuned(df_100_100_5, 'San Francisco CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50_50_5 = create_clusters_tuned(which_min_cluster_size = 50, \n",
    "                                   which_min_samples = 50, \n",
    "                                   which_cluster_selection_epsilon = .00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_clusters_tuned(df_50_50_5, 'San Francisco CA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose largest/most central cluster in each city for downtown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map block groups/dissemination areas to clusters. (HOW???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_only['count'] = 1\n",
    "# grouped_clust = clusters_only.groupby(['id','cluster'])['count'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge the count data back into the original DataFrame\n",
    "# merged_df = pd.merge(clusters_only.drop(columns=['count']), grouped_clust, on=['id', 'cluster'], how='left')\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # are there IDs with multiple clusters?\n",
    "# multiple_clust = merged_df.join(merged_df.groupby('id')['cluster'].nunique(), on='id', rsuffix='_r').sort_values(by=['cluster_r', 'id'], ascending=False)\n",
    "# multiple_clust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # look at one in the US\n",
    "# multiple_clust[multiple_clust['place'].str.contains('\\s\\w{2}$')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMEMBER TO SELECT TWO IN TORONTO CMA -- ONE FOR TORONTO AND ONE FOR MISSISSAUGA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all downtown polygons and export as a single shapefile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
