{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelleong/opt/anaconda3/envs/skmob/lib/python3.7/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.9.1-CAPI-1.14.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely.wkt\n",
    "import json      \n",
    "import requests\n",
    "import gzip\n",
    "import haversine\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "import hdbscan\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## read files ##\n",
    "################\n",
    "\n",
    "#replace LODES files here\n",
    "\n",
    "#LODES WAC file - in this case, it was pre-aggregated by block group instead of blocks\n",
    "dmv_wac_2019 = pd.read_csv('/Users/michaelleong/MIT Documents/WMATA - Future of Work and Climate Goals/dmv_wac_2019.csv')\n",
    "\n",
    "#Census Block Group Shapefile\n",
    "cbg_cbsa_2010 = gpd.read_file('/Users/michaelleong/MIT Documents/WMATA - Future of Work and Climate Goals/cbg_cbsa_2010.shp')\n",
    "\n",
    "#CBSA/CSA Shapefile (for filtering)\n",
    "us_cbsa_geo_2010 = gpd.read_file('/Users/michaelleong/MIT Documents/WMATA - Future of Work and Climate Goals/cb_2019_us_cbsa_500k/cb_2019_us_cbsa_500k.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## scatter random points in polygon ##\n",
    "######################################\n",
    "\n",
    "def Random_Points_in_Bounds(polygon, number):   \n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    x = np.random.uniform(minx, maxx, number*4)\n",
    "    y = np.random.uniform(miny, maxy, number*4)\n",
    "    gdf_poly = gpd.GeoDataFrame(index=[\"myPoly\"], geometry=[polygon])\n",
    "    df = pd.DataFrame()\n",
    "    df['points'] = list(zip(x,y))\n",
    "    df['points'] = df['points'].apply(shapely.geometry.Point)\n",
    "    gdf_points = gpd.GeoDataFrame(df, geometry='points')\n",
    "    Sjoin = gpd.sjoin(gdf_points, gdf_poly, op=\"within\", how='left')\n",
    "    pnts_in_poly = gdf_points[Sjoin.index_right=='myPoly']\n",
    "    return pnts_in_poly['points'].tolist()[0:number]\n",
    "\n",
    "#filter CBGs within CBSA (DC in this case)\n",
    "cbg_cbsa_2010 = cbg_cbsa_2010.rename(columns={'STATEFP':'GEOID10'})\n",
    "cbg_cbsa_2010['GEOID10'] = cbg_cbsa_2010['GEOID10'].astype(int)\n",
    "dmv_cbg_cbsa_2010 = cbg_cbsa_2010[cbg_cbsa_2010['CBSA_NAME']=='Washington-Arlington-Alexandria, DC-VA-MD-WV'].reset_index(drop=True)\n",
    "\n",
    "#join CBGs with LODES\n",
    "dmv_cbg_cbsa_2010 = dmv_cbg_cbsa_2010.merge(dmv_wac_2019, how=\"left\", left_on='GEOID10', right_on='work_cbg_geoid')\n",
    "\n",
    "#calculate employment density (extra variable)\n",
    "dmv_cbg_cbsa_2010['employment_density'] = dmv_cbg_cbsa_2010['C000']/(dmv_cbg_cbsa_2010['ALAND']/4047)\n",
    "\n",
    "#randomly scatter 1 point in each CBG geo for every 100 jobs (this can be modified)\n",
    "dmv_cbg_cbsa_2010['jobs_hundreds'] = round((dmv_cbg_cbsa_2010['C000']/100), 0)\n",
    "dmv_cbg_cbsa_2010 = dmv_cbg_cbsa_2010[dmv_cbg_cbsa_2010['jobs_hundreds']>0].copy()\n",
    "dmv_cbg_cbsa_2010['points'] = dmv_cbg_cbsa_2010[['geometry', 'jobs_hundreds']].apply(lambda x: Random_Points_in_Bounds(x[0], int(x[1])), axis=1)\n",
    "dmv_cbg_cbsa_2010_points = dmv_cbg_cbsa_2010.explode('points')\n",
    "\n",
    "#initialize format for dbscan\n",
    "dmv_cbg_cbsa_2010_dbscan = gpd.GeoDataFrame(dmv_cbg_cbsa_2010_points[['GEOID10', 'points','employment_density']].copy(), geometry='points')\n",
    "dmv_cbg_cbsa_2010_dbscan['point_lon'] = dmv_cbg_cbsa_2010_dbscan['points'].x\n",
    "dmv_cbg_cbsa_2010_dbscan['point_lat'] = dmv_cbg_cbsa_2010_dbscan['points'].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "## apply HDBSCAN ##\n",
    "###################\n",
    "\n",
    "#minimum cluster size can be parameterized. Some literature suggests this be 10,000 jobs (i.e. min_cluster_size = 100).\n",
    "\n",
    "X = StandardScaler().fit_transform(dmv_cbg_cbsa_2010_dbscan[['point_lon', 'point_lat']].copy())\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=25)\n",
    "clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of labels\n",
    "\n",
    "np.max(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append results to original database\n",
    "\n",
    "dmv_cbg_cbsa_2010_dbscan['cluster'] = clusterer.labels_\n",
    "dmv_cbg_cbsa_2010_dbscan.to_csv('dmv_clusters_2500_v1.csv')\n",
    "\n",
    "#remove non-clustered points\n",
    "dmv_cbg_cbsa_2010_dbscan_by_cbg = dmv_cbg_cbsa_2010_dbscan[dmv_cbg_cbsa_2010_dbscan[\"cluster\"]!=-1].groupby(['GEOID10','cluster']).count().reset_index()\n",
    "\n",
    "#create mapping of CBGs to subcenters (drop duplicates such that once one CBG point is captured, that CBG is caputred)\n",
    "#feel free to modify this such that CBGs are only counted if they hit a certain threshold of points, etc.\n",
    "dmv_cbg_cbsa_2010_dbscan_by_cbg = dmv_cbg_cbsa_2010_dbscan_by_cbg.sort_values(by=['GEOID10','point_lon'], ascending=False).drop_duplicates(['GEOID10'], keep='first')[['GEOID10','cluster']].reset_index(drop=True)\n",
    "dmv_cbg_cbsa_2010_dbscan_by_cbg_map = dict(zip(dmv_cbg_cbsa_2010_dbscan_by_cbg['GEOID10'],dmv_cbg_cbsa_2010_dbscan_by_cbg['cluster']))\n",
    "dmv_cbg_cbsa_2010['cluster'] = dmv_cbg_cbsa_2010['GEOID10'].map(dmv_cbg_cbsa_2010_dbscan_by_cbg_map)\n",
    "dmv_cbg_cbsa_2010[['GEOID10','cluster','geometry']].to_csv('dmv_subcenters_2500_v1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fad743816a58aaff4c64981fa0bd82666ad4e0013df4a90a79fa31f7b44ecd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
