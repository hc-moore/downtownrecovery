import boto3, sys, os, csv, re, io
import awswrangler as wr
import pyarrow
import pandas as pd
import numpy as np
import pdb

with open(os.path.expanduser('~/data/downtownrecovery/hannah.moore@utoronto.ca_accessKeys.csv'), newline='') as f:
    reader = csv.reader(f)
    credentials = [cred for cred in reader]
BUCKET_NAME = 'covid19safegraphdatabricks' # replace with your bucket name
ACCESS_KEY = credentials[1][0]
SECRET_KEY = credentials[1][1]
REGION = 'us-west-2'

DOWNLOAD_LOCATION_PATH = "~/data/downtownrecovery/"

s3 = boto3.resource('s3',
                             aws_access_key_id=ACCESS_KEY,
                             aws_secret_access_key=SECRET_KEY,
                  region_name = REGION)

s3_client = boto3.client('s3',
                             aws_access_key_id=ACCESS_KEY,
                             aws_secret_access_key=SECRET_KEY,
                  region_name = REGION)


s3_resource = s3.resource('s3',
                             aws_access_key_id=ACCESS_KEY,
                             aws_secret_access_key=SECRET_KEY,
                  region_name = REGION)

s3_session = boto3.Session(aws_access_key_id=ACCESS_KEY,
                             aws_secret_access_key=SECRET_KEY,
                  region_name = REGION)

s3_bucket = s3.Bucket(BUCKET_NAME)

# Read single parquet file from S3
def pd_read_s3_parquet(key, bucket, **args):
    buffer = io.BytesIO()
    obj = s3.Object(bucket, key)
    obj.download_fileobj(buffer)
    # event_date_string = re.split(r'\/', key)[2]
    single_df = pd.read_parquet(buffer)
    # single_df['event_date'] = event_date_string
    return single_df

# Read multiple parquets from a folder on S3 generated by spark
def pd_read_s3_multiple_parquets(filepath, bucket, verbose=False, **args):
    if not filepath.endswith('/'):
        filepath = filepath + '/'  # Add '/' to the end
 
    s3_keys = [item.key for item in s3.Bucket(bucket).objects.filter(Prefix=filepath)]
    if not s3_keys:
        print('No parquet found in', bucket, filepath)
    elif verbose:
        print('Load parquets:')
        for p in s3_keys: 
            print(p)
    dfs = [pd_read_s3_parquet(key, bucket=bucket, **args) 
           for key in s3_keys[1:]]
    return pd.concat(dfs, ignore_index=True)

KEY = 'oregon-prod/3181607526002649/user/hive/warehouse/' # replace with your object key
# this works best if the tables you want to download are all the same type
# so the loop can just iterate through
# the filetype is rarely discernible from the name, so you need to know what you're downloading
# in AWS before selecting them
table_names = ['lq_clusters_single_period_1015', 'rq_dwtn_clusters_0822', 'rq_city_clusters_0822', 'rq_dwtn_clusters_1015_period_1', 'rq_dwtn_clusters_1015_period_2_']
# df = pd_read_s3_multiple_parquets(KEY, BUCKET_NAME)

for table in table_names:

    key_string = KEY + table + '/'

    # objects = s3_bucket.objects.filter(Prefix = key_string)
    test_df = wr.s3.read_csv(path = 's3://' + BUCKET_NAME + '/' + key_string, path_suffix = ".csv", ignore_empty = True, boto3_session = s3_session, header = None, names = ['city', 'cluster'])
    test_df.to_csv(DOWNLOAD_LOCATION_PATH + 'recovery_clusters/' + table + '.csv')
    
    # test_df = pd_read_s3_multiple_parquets(key_string, bucket = BUCKET_NAME)
    #for i, file in enumerate(objects):
    #    if ".csv" in file.key:
    
    #        csv_obj = s3_client.get_object(Bucket = BUCKET_NAME, Key = file.key)
    #        body = csv_obj['Body']
    #        csv_string = body.read().decode('utf-8')

    #       df = pd.read_csv(io.StringIO(csv_string), names = ['placekey', 'raw_visit_counts', 'normalized_visits_by_total_visits', 'normalized_visits_by_state_scaling', 'poi_cbg', 'date_range_start'])
    #        df_list.append(df)
test_df.shape

test_df.head()
full_df =  pd.concat(df_list)
full_df.to_csv(DOWNLOAD_LOCATION_PATH + table[:-1] + '.csv')


    #contents = test_df['Body'].read().decode('UTF-8')
    #cols = contents.split('\n')[0].split(',')
    #pdb.set_trace()
            #df_list = df_list.append(test_df)
           
            #s3.download_file(BUCKET_NAME, file.key, DOWNLOAD_LOCATION_PATH +  folder + '/' + file.key)
    #df.to_csv(DOWNLOAD_LOCATION_PATH +  folder + '.csv')
    
    
    
test_df = pd.read_csv(DOWNLOAD_LOCATION_PATH +  folder + '_' + str(3) + '.csv', index_col = False)
